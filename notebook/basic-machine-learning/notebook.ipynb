{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b9732316",
   "metadata": {},
   "source": [
    "# <div align=\"center\"><strong>Giới thiệu về Feature Engineering</strong></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efe79025",
   "metadata": {},
   "source": [
    "### __1. Giới thiệu__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84353743",
   "metadata": {},
   "source": [
    "<div align=\"justify\">\n",
    "\n",
    "Trong Machine Learning, các điểm dữ liệu được biểu diễn bằng các vector, được gọi là feature vector hay vector đặc trưng, có độ dài bằng nhau, và cùng là vector cột hoặc vector hàng. Tuy nhiên, trong các bài toán thực tế, mọi chuyện không được tốt đẹp như vậy!\n",
    "\n",
    "Với các bài toán về Computer Vision, các bức ảnh là các ma trận có kích thước khác nhau. Thậm chí để nhận dạng vật thể trong ảnh, ta cần thêm một bước nữa là object detection, tức là tìm cái khung chứa vật thể chúng ta cần dự đoán. Ví dụ, trong bài toán nhận dạng khuôn mặt, chúng ta cần tìm được vị trí các khuôn mặt trong ảnh và crop các khuôn mặt đó trước khi làm các bước tiếp theo. Ngay cả khi đã xác định được các khung chứa các khuôn mặt (và có thể resize các khung đó về cùng một kích thước), ta vẫn phải làm rất nhiều việc nữa vì hình ảnh của khuôn mặt còn phụ thưộc vào góc chụp, ánh sáng, … và rất nhiều yếu tố khác nữa.\n",
    "\n",
    "Các bài toán NLP (Natural Language Processing - Xử lý ngôn ngữ tự nhiên) cũng có khó khăn tương tự khi độ dài của các văn bản là khác nhau, thậm chí có những từ rất hiếm gặp hoặc không có trong từ điển. Cũng có khi thêm một vài từ vào văn bản mà nội dung của văn bản không đổi hoặc hoàn toàn mang nghĩa ngược lại. Hoặc cùng là một câu nói nhưng tốc độ, âm giọng của mỗi người là khác nhau, thậm chí của cùng một người nhưng lúc ốm lúc khỏe cũng khác nhau.\n",
    "\n",
    "Khi làm việc với các bài toán Machine Learning thực tế, nhìn chung chúng ta chỉ có được dữ liệu thô (raw) chưa qua chỉnh sửa, chọn lọc. Chúng ta cần phải tìm một phép biến đổi để loại ra những dữ liệu nhiễu (noise), và để đưa dữ liệu thô với số chiều khác nhau về cùng một chuẩn (cùng là các vector hoặc ma trận). Dữ liệu chuẩn mới này phải đảm bảo giữ được những thông tin đặc trưng (features) cho dữ liệu thô ban đầu. Không những thế, tùy vào từng bài toán, ta cần thiết kế những phép biến đổi để có những features phù hợp. Quá trình quan trọng này được gọi là __Feature Extraction__, hoặc __Feature Engineering__, một số tài liệu tiếng Việt gọi nó là trích chọn đặc trưng.\n",
    "\n",
    "Để giúp các bạn có cái nhìn tổng quan hơn, trong phần tiếp theo tôi xin đặt bước Feature Engineering này trong một bức tranh lớn hơn.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08203b41",
   "metadata": {},
   "source": [
    "### __2. Mô hình chung cho các bài toán Machine Learning__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b26d953",
   "metadata": {},
   "source": [
    "<div align=\"justify\">\n",
    "\n",
    "Phần lớn các bài toán Machine Learning có thể được thể hiện trong hình vẽ dưới đây:\n",
    "\n",
    "<div align=\"center\">\n",
    "\n",
    "![](../../images/general-workflow-for-machine-learning.png)\n",
    "\n",
    "__Hình 1:__ Mô hình chung cho các bài toán Machine Learning\n",
    "\n",
    "</div>\n",
    "\n",
    "Có hai phases lớn là Training phase và Testing phase. Xin nhắc lại là với các bài toán Supervised learning, ta có các cặp dữ liệu (input, output), với các bài toán Unsupervised learing, ta chỉ có input mà thôi.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c69c91b",
   "metadata": {},
   "source": [
    "#### __Training Phase__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed97b5e8",
   "metadata": {},
   "source": [
    "<div align=\"justify\">\n",
    "\n",
    "Có hai khối có nền màu xanh lục chúng ta cần phải thiết kế:\n",
    "\n",
    "<span style=\"color: red\"><strong>Feature Extractor</strong></span>\n",
    "\n",
    "__Đầu ra:__ Tôi xin đề cập đầu ra của khối này trước vì mục đích của Feature Engineering là tạo ra một Feature Extractor biến dữ liệu thô ban đầu thành dữ liệu phù hợp với từng mục đích khác nhau.\n",
    "\n",
    "__Đầu vào:__\n",
    "\n",
    "- __raw training input:__ Raw input là tất cả các thông tin ta biết về dữ liệu. Ví dụ: với ảnh thì là giá trị của từng pixel; với văn bản thì là từng từ, từng câu; với file âm thanh thì nó là một đoạn tín hiệu; với cơ sở dữ liệu Iris thì nó là độ dài các cánh hoa và đài hoa, … Dữ liệu thô này thường không ở dạng vector, không có số chiều như nhau. Thậm chí có thể có số chiều như nhau nhưng số chiều quá lớn, như một bức ảnh màu 1000 pixel x 1000 pixel thì số elements đã là $3 \\times 10^6$ (3 vì ảnh màu thường có 3 channels: Red, Green, Blue). Đây là một con số quá lớn, không lợi cho lưu trữ và tính toán.\n",
    "- __(optional) output của training set:__ Trong các bài toán Unsupervised learning, ta không biết output nên hiển nhiên sẽ không có đầu vào này. Trong các bài toán Supervised learning, có khi dữ liệu này cũng không được sử dụng. Ví dụ: nếu raw input đã có cùng số chiều rồi nhưng số chiều quá lớn, ta muốn giảm số chiều của nó thì cách đơn giản nhất là chiếu vector đó xuống một không gian có số chiều nhỏ hơn bằng cách lấy một ma trận ngẫu nhiên nhân với nó. Ma trận này thường là ma trận béo (số hàng ít hơn số cột, tiếng Anh - fat matrices) để đảm bảo số chiều thu được nhỏ hơn số chiều ban đầu. Việc làm này mặc dù làm mất đi thông tin, trong nhiều trường hợp vẫn mang lại hiệu quả vì đã giảm được lượng tính toán ở phần sau. Đôi khi ma trận chiếu không phải là ngẫu nhiên mà có thể được học dựa trên toàn bộ raw input, ta sẽ có bài toán tìm ma trận chiếu để lượng thông tin mất đi là ít nhất. Trong nhiều trường hợp, dữ liệu output của training set cũng được sử dụng để tạo ra Feature Extractor. Ví dụ: trong bài toán classification, ta không quan tâm nhiều đến việc mất thông tin hay không, ta chỉ quan tâm đến việc những thông tin còn lại có đặc trưng cho từng class hay không. Ví dụ, dữ liệu thô là các hình vuông và hình tam giác có màu đỏ và xanh. Trong bài toán phân loại đa giác, các output là tam giác và vuông, thì ta không quan tâm tới màu sắc mà chỉ quan tâm tới số cạnh của đa giác. Ngược lại, trong bài toán phân loại màu, các class là xanh và đỏ, ta không quan tâm tới số cạnh mà chỉ quan tâm đến màu sắc thôi.\n",
    "- __(optional) Prior knowledge about data:__ Đôi khi những giả thiết khác về dữ liệu cũng mang lại lợi ích. Ví dụ, trong bài toán classification, nếu ta biết dữ liệu là (gần như) linearly separable thì ta sẽ đi tìm một ma trận chiếu sao cho ở trong không gian mới, dữ liệu vẫn đảm bảo tính linearly separable, việc này thuận tiện hơn cho phần classification vì các thuật toán linear, nhìn chung, đơn giản hơn.\n",
    "\n",
    "Sau khi học được feature extractor thì ta cũng sẽ thu được extracted features cho raw input data. Những extracted features này được dùng để huấn luyện các thuật toán Classification, Clustering, Regression,… ở phía sau.\n",
    "\n",
    "<span style=\"color: red\"><strong>Main Algorithms</strong></span>\n",
    "\n",
    "Khi có được extracted features rồi, chúng ta sử dụng những thông tin này cùng với (optional) training output và (optional) prior knowledge để tạo ra các mô hình phù hợp, điều mà chúng ta đã làm ở những bài trước.\n",
    "\n",
    "__Chú ý:__ Trong một số thuật toán cao cấp hơn, việc huấn luyện feature extractor và main algorithm được thực hiện cùng lúc với nhau chứ không phải từng bước như trên.\n",
    "\n",
    "_Một điểm rất quan trọng: khi xây dựng bộ feature extractor và main algorithms, chúng ta không được sử dụng bất kỳ thông tin nào trong tập test data. Ta phải giả sử rằng những thông tin trong test data chưa được nhìn thấy bao giờ. Nếu sử dụng thêm thông tin về test data thì rõ ràng ta đã ăn gian! Tôi từng đánh giá các bài báo khoa học quốc tế, rất nhiều tác giả xây dựng mô hình dùng cả dữ liệu test data, sau đó lại dùng chính mô hình đó để kiểm tra trên test data đó. Việc ăn gian này là lỗi rất nặng và hiển nhiên những bài báo đó bị từ chối (reject)._\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42e8a578",
   "metadata": {},
   "source": [
    "#### __Testing Phase__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c85b9ccb",
   "metadata": {},
   "source": [
    "<div align=\"justify\">\n",
    "\n",
    "Bước này đơn giản hơn nhiều. Với raw input mới, ta sử dụng feature extractor đã tạo được ở trên (tất nhiên không được sử dụng output của nó vì output là cái ta đang đi tìm) để tạo ra feature vector tương ứng. Feature vector được đưa vào main algorithm đã được học ở training phase để dự đoán output.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3221d431",
   "metadata": {},
   "source": [
    "### __3. Một số ví dụ về Feature Engineering__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b626eb2",
   "metadata": {},
   "source": [
    "#### __Trực tiếp lấy raw data__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "949a31c3",
   "metadata": {},
   "source": [
    "<div align=\"justify\">\n",
    "\n",
    "Với bài toán phân loại chữ số viết tay trong bộ cơ sở dữ liệu MNIST, mỗi bức ảnh có số chiều là 28 pixel x 28 pixel (tất nhiên việc crop và chỉnh sửa mỗi bức ảnh đã được thực hiện từ trước rồi, đó đã là một phần của feature engineering rồi). Một cách đơn giản thường được dùng là kéo dài ma trận 28x28 này để được 1 vector có số chiều 784. Trong cách này, các cột (hoặc hàng) của ma trận ảnh được đặt chồng lên (hoặc cạnh nhau) để được 1 vector dài. Vector dài này được trực tiếp sử dụng làm feature đưa vào các bộ classifier/clustering/regression/… Lúc này, giá trị của mỗi pixel ảnh được coi là một feature.\n",
    "\n",
    "Rõ ràng việc làm đơn giản này đã làm mất thông tin về không gian (spatial information) giữa các điểm ảnh, tuy nhiên, trong nhiều trường hợp, nó vẫn mang lại kết quả khả quan.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8ac2063",
   "metadata": {},
   "source": [
    "#### __Lựa chọn đặc trưng__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2717060d",
   "metadata": {},
   "source": [
    "<div align=\"justify\">\n",
    "\n",
    "Giả sử rằng các điểm dữ liệu có số features khác nhau (do kích thước dữ liệu khác nhau hay do một số feature mà điểm dữ liệu này có nhưng điểm dữ liệu kia lại không thu thập được), và số lượng features là cực lớn. Chúng ta cần chọn ra một số lượng nhỏ hơn các feature phù hợp với bài toán. Chọn thế nào và thế nào là phù hợp lại là một bài toán khác, tôi sẽ không bàn thêm ở đây.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c61ce2c",
   "metadata": {},
   "source": [
    "#### __Dimensionality reduction__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf7a03cf",
   "metadata": {},
   "source": [
    "<div align=\"justify\">\n",
    "\n",
    "Một phương pháp nữa tôi đã đề cập đó là làm giảm số chiều của dữ liệu để giảm bộ nhớ và khối lượng tính toán. Việc giảm số chiều này có thể được thực hiện bằng nhiều cách, trong đó random projection là cách đơn giản nhất. Tức chọn một ma trận chiếu (projection matrix) ngẫu nhiên (ma trận béo) rồi nhân nó với từng điểm dữ liệu (giả sử dữ liệu ở dạng vector cột) để được các vector có số chiều thấp hơn. Ví dụ, vector ban đầu có số chiều là 784, chọn ma trận chiếu có kích thước (100x784), khi đó nếu nhân ma trận chéo này với vector ban đầu, ta sẽ được một vector mới có số chiều là 100, nhỏ hơn số chiều ban đầu rất nhiều. Lúc này, có thể ta không có tên gọi cho mỗi feature nữa vì các feature ở vector ban đầu đã được trộn lẫn với nhau theo một tỉ lệ nào đó rồi lưu vào vector mới này. Mỗi thành phần của vector mới này được coi là một feature (không tên).\n",
    "\n",
    "Việc chọn một ma trận chiếu ngẫu nhiên đôi khi mang lại kết quả tệ không mong muốn vì thông tin bị mất đi quá nhiều. Một phương pháp được sử dụng nhiều để hạn chế lượng thông tin mất đi có tên là Principle Component Analysis sẽ được tôi trình bày sau đây khoảng 1-2 tháng.\n",
    "\n",
    "__Chú ý:__ Feature learning không nhất thiết phải làm giảm số chiều dữ liệu, đôi khi feature vector còn có số chiều lớn hơn raw data. Random projection cũng có thể làm được việc này nếu ma trận chiếu là một ma trận cao (số cột ít hơn số hàng).\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4c89277",
   "metadata": {},
   "source": [
    "#### __Bag-of-words__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2c28328",
   "metadata": {},
   "source": [
    "<div align=\"justify\">\n",
    "\n",
    "Hẳn rất nhiều bạn đã tự đặt câu hỏi: Với một văn bản thì feature vector sẽ có dạng như thế nào? Làm sao đưa các từ, các câu, đoạn văn ở dạng text trong các văn bản về một vector mà mỗi phần tử là một số?\n",
    "\n",
    "Có một phương pháp rất phổ biến giúp bạn trả lời những câu hỏi này. Phương pháp đó có tên là Bag of Words (BoW) (Túi đựng Từ).\n",
    "\n",
    "Vẫn theo thói quen, tôi bắt đầu bằng một ví dụ. Giả sử chúng ta có bài toán phân loại tin rác. Ta thấy rằng nếu một tin có chứa các từ khuyến mại, giảm giá, trúng thưởng, miễn phí, quà tặng, tri ân, … thì nhiều khả năng đó là một tin nhắn rác. Vậy phương pháp đơn giản nhất là đếm xem trong tin đó có bao nhiêu từ thuộc vào các từ trên, nếu nhiều hơn 1 ngưỡng nào đó thì ta quyết định đó là tin rác. (Tất nhiên bài toán thực tế phức tạp hơn nhiều khi các từ có thể được viết dưới dạng không dấu, hoặc bị cố tình viết sai chính tả, hoặc dùng ngôn ngữ teen). Với các loại văn bản khác nhau thì lượng từ liên quan tới từng chủ đề cũng khác nhau. Từ đó có thể dựa vào số lượng các từ trong từng loại để làm các vector đặc trưng cho từng văn bản.\n",
    "\n",
    "Tôi xin lấy ví dụ cụ thể hơn về cách tạo ra vector đặc trưng cho mỗi văn bản dựa trên BoW và xin được lấy tiếng Anh làm ví dụ (nguồn Bag of Words wiki. Tiếng Việt khó hơn vì một từ có thể có nhiều âm tiết, tiếng Anh thì thường cứ gặp dấu cách là kết thúc một từ).\n",
    "\n",
    "Giả sử chúng ta có hai văn bản đơn giản:\n",
    "\n",
    "```\n",
    "(1) John likes to watch movies. Mary likes movies too.\n",
    "```\n",
    "\n",
    "và\n",
    "\n",
    "```\n",
    "(2) John also likes to watch football games.\n",
    "```\n",
    "\n",
    "Dựa trên hai văn bản này, ta có danh sách các từ được sử dụng, được gọi là từ điển với 10 từ như sau:\n",
    "\n",
    "```\n",
    "[\"John\", \"likes\", \"to\", \"watch\", \"movies\", \"also\", \"football\", \"games\", \"Mary\", \"too\"]\n",
    "```\n",
    "\n",
    "Với mỗi văn bản, ta sẽ tạo ra một vector đặc trưng có số chiều bằng 10, mỗi phần tử đại diện cho số từ tương ứng xuất hiện trong văn bản đó. Với hai văn bản trên, ta sẽ có hai vector đặc trưng là:\n",
    "\n",
    "```\n",
    "(1) [1, 2, 1, 1, 2, 0, 0, 0, 1, 1]\n",
    "(2) [1, 1, 1, 1, 0, 1, 1, 1, 0, 0]\n",
    "```\n",
    "\n",
    "Văn bản (1) có 1 từ “John”, 2 từ “likes”, 0 từ “also”, 0 từ “football”, … nên ta thu được vector tương ứng như trên.\n",
    "\n",
    "Có một vài điều cần lưu ý trong BoW:\n",
    "\n",
    "- Với những ứng dụng thực tế, từ điền có nhiều hơn 10 từ rất nhiều, có thể đến một trăm nghìn hoặc cả triệu, như vậy vector đặc trưng thu được sẽ rất dài. Một văn bản chỉ có 1 câu, và 1 tiểu thuyết nghìn trang đều được biểu diễn bằng các vector có số chiều bằng 100 nghìn hoặc 1 triệu.\n",
    "- Có rất nhiều từ trong từ điển không xuất hiện trong một văn bản. Như vậy các vector đặc trưng thu được thường có rất nhiều phần tử bằng 0. Các vector có nhiều phần tử bằng 0 được gọi là sparse vector (sparse hiểu theo nghĩa là thưa thớt, rải rác, tôi xin phép chỉ sử dụng khái niệm này bằng tiếng Anh). Để việc lưu trữ được hiệu quả hơn, ta không lưu cả vector đó mà chỉ lưu vị trí của các phần tử khác 0 và giá trị tương ứng. Lưu ý: nếu có hơn 50% số phần tử khác 0, việc làm này lại phản tác dụng!\n",
    "- Thi thoảng có những từ hiếm gặp không nằm trong từ điển, vậy ta sẽ làm gì? Một cách thường được dùng là mở rộng vector đặc trưng thêm 1 phần tử, gọi là phẩn tử <Unknown>. Mọi từ không có trong từ điền đều được coi là <Unknown>.\n",
    "- Nghĩ kỹ một chút, những từ hiếm đôi khi lại mang những thông tin quan trọng nhất mà chỉ loại văn bản đó có. Đây là một nhược điểm của BoW. Có một phương pháp cải tiến khác giúp khắc phục nhược điểm này có tên là Term Frequency-Inverse Document Frequency (TF-IDF) dùng để xác định tầm quan trọng của một từ trong một văn bản dựa trên toàn bộ văn bản trong cơ sở dữ liệu (corpus). Bạn đọc muốn tìm hiểu thêm có thể xem 5 Algorithms Every Web Developer Can Use and Understand, section 5.\n",
    "- Nhược điểm lớn nhất của BoW là nó không mang thông tin về thứ tự của các từ. Cũng như sự liên kết giữa các câu, các đoạn văn trong văn bản. Ví dụ, ba câu sau đây: “Em yêu anh không?”, “Em không yêu anh”, và “Không, (nhưng) anh yêu em” khi được trích chọn đặc trưng bằng BoW sẽ cho ra ba vector giống hệt nhau, mặc dù ý nghĩa khác hẳn nhau.\n",
    "\n",
    "__Bonus:__ hình dưới đây là tần suất sử dụng các từ (coi mỗi âm tiết là một từ) trong Truyện Kiều (theo bản này) nếu ta chỉ sử dụng 30 từ có tần suất cao nhất.\n",
    "\n",
    "<div align=\"center\">\n",
    "\n",
    "![](../../images/bow-example.png)\n",
    "\n",
    "__Hình 2:__ Bag of Words cho Truyện Kiều với 30 từ có tần suất cao nhất.\n",
    "\n",
    "</div>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8983696",
   "metadata": {},
   "source": [
    "#### __Bag-of-Words trong Computer Vision__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d6ad07b",
   "metadata": {},
   "source": [
    "<div align=\"justify\">\n",
    "\n",
    "Bags of Words cũng được áp dụng trong Computer Vision với cách định nghĩa words và từ điển khác.\n",
    "\n",
    "Xét các ví dụ sau:\n",
    "\n",
    "<strong>Ví dụ 1</strong>\n",
    "\n",
    "Có hai class ảnh, một class là ảnh các khu rừng, một class là ảnh các sa mạc. Phân loại một bức ảnh là rừng hay sa mạc (giả sử ta biết rằng nó thuộc một trong hai loại này) một cách trực quan nhất là dựa vào màu sắc. Màu xanh nhiều thì là rừng, màu đỏ và vàng nhiều thì là sa mạc. Vậy chúng ta có thể có một mô hình đơn giản để trích chọn đặc trưng như sau:\n",
    "\n",
    "- Với một bức ảnh, chuẩn bị một vector $ \\mathbf{x} $ có số chiều bằng 3, đại diện cho 3 màu: xanh ($ x_1 $), đỏ ($ x_2 $), và vàng ($ x_3 $).\n",
    "- Với mỗi điểm ảnh trong bức ảnh đó, xem nó gần với màu xanh, đỏ hay vàng nhất dựa trên giá trị của pixel đó. Nếu nó gần điểm xanh nhất, tăng $ x_1 $ lên 1. Nếu gần đỏ nhất, tăng $ x_2 $ lên 1. Nếu gần vàng nhất, tăng $ x_3 $ lên 1.\n",
    "- Sau khi xem xét tất cả các điểm ảnh, dù cho bức ảnh có kích thước thế nào, ta vẫn thu được một vector có độ dài bằng 3, mỗi phần tử thể hiện việc có bao nhiêu pixel trong bức ảnh có màu tương ứng. Vector cuối này còn được gọi là vector histogram của bức ảnh tương ứng với ba màu xanh, đỏ, vàng. Dựa vào vector này, ta có thể quyết định bức ảnh đó là ảnh rừng hay sa mạc.\n",
    "\n",
    "<strong>Ví dụ 2</strong>\n",
    "\n",
    "Trên thực tế, các bài toán xử lý ảnh không đơn giản như ví dụ 1 trên đây. Mắt người thực ra nhạy với các đường nét, hình dáng hơn là màu sắc. Một cái (ảnh) cây dù không có màu vẫn là một cái (ảnh) cây! Vì vậy, xem xét giá trị từng điểm ảnh một không mang lại kết quả khả quan vì lượng thông tin bị mất quá nhiều.\n",
    "\n",
    "Có một cách khắc phục là thay vì xem xét một điểm ảnh, ta xem xét một cửa sổ nhỏ trong ảnh (trong Computer Vision, cửa sổ này được gọi là patch) là một hình chữ nhật chứa nhiều điểm ảnh gần nhau. Cửa sổ này đủ lớn để có thể chứa được các bộ phận có thể mô tả được vật thể trong ảnh.\n",
    "\n",
    "Ví dụ với mặt người, các patch nên đủ lớn để chứa được các phần của khuôn mặt như mắt, mũi, miệng như hình dưới đây.\n",
    "\n",
    "<div align=\"center\">\n",
    "\n",
    "![](../../images/bow_face.png)\n",
    "\n",
    "__Hình 3:__ Bag of Words cho ảnh chứa mặt người. (Nguồn Bag of visual words model: recognizing object categories)\n",
    "\n",
    "</div>\n",
    "\n",
    "Tương tự thế, với ảnh là ô tô, các patch thu được có thể là bánh xe, khung xe, cửa xe, … như hàng trên trong hình dưới đây.\n",
    "\n",
    "<div align=\"center\">\n",
    "\n",
    "![](../../images/bow_car.png)\n",
    "\n",
    "__Hình 4:__ Bag of Words cho ảnh ô tô. (Nguồn: tôi cố gắng tìm nguồn cho hình này nhưng tất cả các tài liệu tôi tìm được đều ghi \"Source: B. Leibe\", tôi cũng xin được trích nguồn tương tự)\n",
    "\n",
    "</div>\n",
    "\n",
    "Có một câu hỏi đặt ra là, trong xử lý văn bản, hai từ được coi là như nhau nếu nó được biểu diễn bởi các ký tự giống nhau. Vậy trong xử lý ảnh, hai patches được coi là như nhau khi nào? Khi mọi pixel trong hai patches có giá trị bằng nhau sao?\n",
    "\n",
    "Câu trả lời là không. Xác suất để hai patches giống hệt nhau từng pixel là rất thấp vì có thể một phần của vật thể trong một patch bị lệch đi vài pixel so với phần đó trong patch kia; hoặc phần vật thể trong patch bị méo, hoặc có độ sáng khác nhau, mặc dù ta vẫn nhìn thấy hai patches đó rất giống nhau. Vậy thì hai patch được coi là như nhau khi nào? Và từ điển ở đây được định nghĩa như thế nào?\n",
    "\n",
    "__Câu trả lời ngắn:__ hai patches là gần giống nhau nếu khoảng cách Euclid giữa hai vector tạo bởi hai patches đó gần nhau. Từ điển (codebook) sẽ có số phần tử do ta tự chọn. Số phần tử càng cao thì độ sai lệch càng ít, nhưng sẽ nặng về tính toán hơn.\n",
    "\n",
    "__Câu trả lời dài:__ chúng ta có thể áp dụng K-means clustering. Với rất nhiều patches thu được, giả sử ta muốn xây dựng một codebook với chỉ khoảng 1000 words. Vậy thì ta cho k = 1000 rồi thực hiện K-means clustering trên toàn bộ số patches thu được (từ tập training). Sau khi thực hiện K-means clustering, ta thu được 1000 clusters và 1000 centers tương ứng. Mỗi centers này được coi là một words, và tất cả những điểm rơi vào cùng một cluster được coi là cùng một bag. Với ảnh trong tập test data, ta cũng lấy các patches rồi xem chúng rơi vào những bags nào. Từ đó suy ra vector đặc trưng cho mỗi bức ảnh. Chú ý rằng với k = 1000, mỗi bức ảnh sẽ được mô tả bởi một vector có số chiều 1000, tức là mỗi điểm dữ liệu bây giờ đã có số chiều bằng nhau, mặc dù ảnh thô đầu vào có thể có kích thước khác nhau.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7aac30e",
   "metadata": {},
   "source": [
    "#### __Feature Scaling and Normalization__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7a60e6e",
   "metadata": {},
   "source": [
    "<div align=\"justify\">\n",
    "\n",
    "Các điểm dữ liệu đôi khi được đo đạc với những đơn vị khác nhau, m và feet chẳng hạn. Hoặc có hai thành phần (của vector dữ liệu) chênh lệch nhau quá lớn, một thành phần có khoảng giá trị từ 0 đến 1000, thành phần kia chỉ có khoảng giá trị từ 0 đến 1 chẳng hạn. Lúc này, chúng ta cần chuẩn hóa dữ liệu trước khi thực hiện các bước tiếp theo.\n",
    "\n",
    "__Chú ý:__ việc chuẩn hóa này chỉ được thực hiện khi vector dữ liệu đã có cùng chiều.\n",
    "\n",
    "Một vài phương pháp chuẩn hóa thường dùng:\n",
    "\n",
    "<span color=\"red\"><strong>Rescaling</strong></span>\n",
    "\n",
    "Phương pháp đơn giản nhất là đưa tất cả các thành phần về cùng một khoảng, $[0, 1]$ hoặc $[-1, 1]$ chẳng hạn, tùy thuộc vào ứng dụng.  \n",
    "Nếu muốn đưa một thành phần (feature) về khoảng $[0, 1]$, công thức sẽ là:\n",
    "\n",
    "$$\n",
    "x' = \\frac{x - \\min(x)}{\\max(x) - \\min(x)}\n",
    "$$\n",
    "\n",
    "trong đó $x$ là giá trị ban đầu, $x'$ là giá trị sau khi chuẩn hóa.  \n",
    "$\\min(x), \\max(x)$ được tính trên toàn bộ dữ liệu training data ở cùng một thành phần.  \n",
    "Việc này được thực hiện trên từng thành phần của vector dữ liệu $\\mathbf{x}$.\n",
    "\n",
    "<span color=\"red\"><strong>Standardization</strong></span>\n",
    "\n",
    "Một phương pháp nữa cũng hay được sử dụng là giả sử mỗi thành phần đều có phân phối chuẩn với kỳ vọng là 0 và phương sai là 1.  \n",
    "Khi đó, công thức chuẩn hóa sẽ là:\n",
    "\n",
    "$$\n",
    "x' = \\frac{x - \\bar{x}}{\\sigma}\n",
    "$$\n",
    "\n",
    "với $\\bar{x}, \\sigma$ lần lượt là kỳ vọng (giá trị trung bình) và độ lệch chuẩn (standard deviation) của thành phần đó trên toàn bộ training data.\n",
    "\n",
    "<span color=\"red\"><strong>Scaling to unit length\n",
    "</strong></span>\n",
    "\n",
    "Một lựa chọn khác nữa cũng được sử dụng rộng rãi là chuẩn hóa các thành phần của mỗi vector dữ liệu sao cho toàn bộ vector có độ lớn (chuẩn Euclid, tức norm bậc 2) bằng 1.  \n",
    "Việc này có thể được thực hiện bằng công thức:\n",
    "\n",
    "$$\n",
    "\\mathbf{x}' = \\frac{\\mathbf{x}}{\\|\\mathbf{x}\\|_2}\n",
    "$$\n",
    "\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
