{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b9732316",
   "metadata": {},
   "source": [
    "# <div align=\"center\"><strong>Giới thiệu về Feature Engineering</strong></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efe79025",
   "metadata": {},
   "source": [
    "### __1. Giới thiệu__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84353743",
   "metadata": {},
   "source": [
    "<div align=\"justify\">\n",
    "\n",
    "Trong Machine Learning, các điểm dữ liệu được biểu diễn bằng các vector, được gọi là feature vector hay vector đặc trưng, có độ dài bằng nhau, và cùng là vector cột hoặc vector hàng. Tuy nhiên, trong các bài toán thực tế, mọi chuyện không được tốt đẹp như vậy!\n",
    "\n",
    "Với các bài toán về Computer Vision, các bức ảnh là các ma trận có kích thước khác nhau. Thậm chí để nhận dạng vật thể trong ảnh, ta cần thêm một bước nữa là object detection, tức là tìm cái khung chứa vật thể chúng ta cần dự đoán. Ví dụ, trong bài toán nhận dạng khuôn mặt, chúng ta cần tìm được vị trí các khuôn mặt trong ảnh và crop các khuôn mặt đó trước khi làm các bước tiếp theo. Ngay cả khi đã xác định được các khung chứa các khuôn mặt (và có thể resize các khung đó về cùng một kích thước), ta vẫn phải làm rất nhiều việc nữa vì hình ảnh của khuôn mặt còn phụ thưộc vào góc chụp, ánh sáng, … và rất nhiều yếu tố khác nữa.\n",
    "\n",
    "Các bài toán NLP (Natural Language Processing - Xử lý ngôn ngữ tự nhiên) cũng có khó khăn tương tự khi độ dài của các văn bản là khác nhau, thậm chí có những từ rất hiếm gặp hoặc không có trong từ điển. Cũng có khi thêm một vài từ vào văn bản mà nội dung của văn bản không đổi hoặc hoàn toàn mang nghĩa ngược lại. Hoặc cùng là một câu nói nhưng tốc độ, âm giọng của mỗi người là khác nhau, thậm chí của cùng một người nhưng lúc ốm lúc khỏe cũng khác nhau.\n",
    "\n",
    "Khi làm việc với các bài toán Machine Learning thực tế, nhìn chung chúng ta chỉ có được dữ liệu thô (raw) chưa qua chỉnh sửa, chọn lọc. Chúng ta cần phải tìm một phép biến đổi để loại ra những dữ liệu nhiễu (noise), và để đưa dữ liệu thô với số chiều khác nhau về cùng một chuẩn (cùng là các vector hoặc ma trận). Dữ liệu chuẩn mới này phải đảm bảo giữ được những thông tin đặc trưng (features) cho dữ liệu thô ban đầu. Không những thế, tùy vào từng bài toán, ta cần thiết kế những phép biến đổi để có những features phù hợp. Quá trình quan trọng này được gọi là __Feature Extraction__, hoặc __Feature Engineering__, một số tài liệu tiếng Việt gọi nó là trích chọn đặc trưng.\n",
    "\n",
    "Để giúp các bạn có cái nhìn tổng quan hơn, trong phần tiếp theo tôi xin đặt bước Feature Engineering này trong một bức tranh lớn hơn.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08203b41",
   "metadata": {},
   "source": [
    "### __2. Mô hình chung cho các bài toán Machine Learning__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b26d953",
   "metadata": {},
   "source": [
    "<div align=\"justify\">\n",
    "\n",
    "Phần lớn các bài toán Machine Learning có thể được thể hiện trong hình vẽ dưới đây:\n",
    "\n",
    "<div align=\"center\">\n",
    "\n",
    "![](../../images/general-workflow-for-machine-learning.png)\n",
    "\n",
    "__Hình 1:__ Mô hình chung cho các bài toán Machine Learning\n",
    "\n",
    "</div>\n",
    "\n",
    "Có hai phases lớn là Training phase và Testing phase. Xin nhắc lại là với các bài toán Supervised learning, ta có các cặp dữ liệu (input, output), với các bài toán Unsupervised learing, ta chỉ có input mà thôi.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c69c91b",
   "metadata": {},
   "source": [
    "#### __Training Phase__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed97b5e8",
   "metadata": {},
   "source": [
    "<div align=\"justify\">\n",
    "\n",
    "Có hai khối có nền màu xanh lục chúng ta cần phải thiết kế:\n",
    "\n",
    "<span style=\"color: red\"><strong>Feature Extractor</strong></span>\n",
    "\n",
    "__Đầu ra:__ Tôi xin đề cập đầu ra của khối này trước vì mục đích của Feature Engineering là tạo ra một Feature Extractor biến dữ liệu thô ban đầu thành dữ liệu phù hợp với từng mục đích khác nhau.\n",
    "\n",
    "__Đầu vào:__\n",
    "\n",
    "- __raw training input:__ Raw input là tất cả các thông tin ta biết về dữ liệu. Ví dụ: với ảnh thì là giá trị của từng pixel; với văn bản thì là từng từ, từng câu; với file âm thanh thì nó là một đoạn tín hiệu; với cơ sở dữ liệu Iris thì nó là độ dài các cánh hoa và đài hoa, … Dữ liệu thô này thường không ở dạng vector, không có số chiều như nhau. Thậm chí có thể có số chiều như nhau nhưng số chiều quá lớn, như một bức ảnh màu 1000 pixel x 1000 pixel thì số elements đã là $3 \\times 10^6$ (3 vì ảnh màu thường có 3 channels: Red, Green, Blue). Đây là một con số quá lớn, không lợi cho lưu trữ và tính toán.\n",
    "- __(optional) output của training set:__ Trong các bài toán Unsupervised learning, ta không biết output nên hiển nhiên sẽ không có đầu vào này. Trong các bài toán Supervised learning, có khi dữ liệu này cũng không được sử dụng. Ví dụ: nếu raw input đã có cùng số chiều rồi nhưng số chiều quá lớn, ta muốn giảm số chiều của nó thì cách đơn giản nhất là chiếu vector đó xuống một không gian có số chiều nhỏ hơn bằng cách lấy một ma trận ngẫu nhiên nhân với nó. Ma trận này thường là ma trận béo (số hàng ít hơn số cột, tiếng Anh - fat matrices) để đảm bảo số chiều thu được nhỏ hơn số chiều ban đầu. Việc làm này mặc dù làm mất đi thông tin, trong nhiều trường hợp vẫn mang lại hiệu quả vì đã giảm được lượng tính toán ở phần sau. Đôi khi ma trận chiếu không phải là ngẫu nhiên mà có thể được học dựa trên toàn bộ raw input, ta sẽ có bài toán tìm ma trận chiếu để lượng thông tin mất đi là ít nhất. Trong nhiều trường hợp, dữ liệu output của training set cũng được sử dụng để tạo ra Feature Extractor. Ví dụ: trong bài toán classification, ta không quan tâm nhiều đến việc mất thông tin hay không, ta chỉ quan tâm đến việc những thông tin còn lại có đặc trưng cho từng class hay không. Ví dụ, dữ liệu thô là các hình vuông và hình tam giác có màu đỏ và xanh. Trong bài toán phân loại đa giác, các output là tam giác và vuông, thì ta không quan tâm tới màu sắc mà chỉ quan tâm tới số cạnh của đa giác. Ngược lại, trong bài toán phân loại màu, các class là xanh và đỏ, ta không quan tâm tới số cạnh mà chỉ quan tâm đến màu sắc thôi.\n",
    "- __(optional) Prior knowledge about data:__ Đôi khi những giả thiết khác về dữ liệu cũng mang lại lợi ích. Ví dụ, trong bài toán classification, nếu ta biết dữ liệu là (gần như) linearly separable thì ta sẽ đi tìm một ma trận chiếu sao cho ở trong không gian mới, dữ liệu vẫn đảm bảo tính linearly separable, việc này thuận tiện hơn cho phần classification vì các thuật toán linear, nhìn chung, đơn giản hơn.\n",
    "\n",
    "Sau khi học được feature extractor thì ta cũng sẽ thu được extracted features cho raw input data. Những extracted features này được dùng để huấn luyện các thuật toán Classification, Clustering, Regression,… ở phía sau.\n",
    "\n",
    "<span style=\"color: red\"><strong>Main Algorithms</strong></span>\n",
    "\n",
    "Khi có được extracted features rồi, chúng ta sử dụng những thông tin này cùng với (optional) training output và (optional) prior knowledge để tạo ra các mô hình phù hợp, điều mà chúng ta đã làm ở những bài trước.\n",
    "\n",
    "__Chú ý:__ Trong một số thuật toán cao cấp hơn, việc huấn luyện feature extractor và main algorithm được thực hiện cùng lúc với nhau chứ không phải từng bước như trên.\n",
    "\n",
    "_Một điểm rất quan trọng: khi xây dựng bộ feature extractor và main algorithms, chúng ta không được sử dụng bất kỳ thông tin nào trong tập test data. Ta phải giả sử rằng những thông tin trong test data chưa được nhìn thấy bao giờ. Nếu sử dụng thêm thông tin về test data thì rõ ràng ta đã ăn gian! Tôi từng đánh giá các bài báo khoa học quốc tế, rất nhiều tác giả xây dựng mô hình dùng cả dữ liệu test data, sau đó lại dùng chính mô hình đó để kiểm tra trên test data đó. Việc ăn gian này là lỗi rất nặng và hiển nhiên những bài báo đó bị từ chối (reject)._\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42e8a578",
   "metadata": {},
   "source": [
    "#### __Testing Phase__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c85b9ccb",
   "metadata": {},
   "source": [
    "<div align=\"justify\">\n",
    "\n",
    "Bước này đơn giản hơn nhiều. Với raw input mới, ta sử dụng feature extractor đã tạo được ở trên (tất nhiên không được sử dụng output của nó vì output là cái ta đang đi tìm) để tạo ra feature vector tương ứng. Feature vector được đưa vào main algorithm đã được học ở training phase để dự đoán output.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3221d431",
   "metadata": {},
   "source": [
    "### __3. Một số ví dụ về Feature Engineering__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b626eb2",
   "metadata": {},
   "source": [
    "#### __Trực tiếp lấy raw data__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "949a31c3",
   "metadata": {},
   "source": [
    "<div align=\"justify\">\n",
    "\n",
    "Với bài toán phân loại chữ số viết tay trong bộ cơ sở dữ liệu MNIST, mỗi bức ảnh có số chiều là 28 pixel x 28 pixel (tất nhiên việc crop và chỉnh sửa mỗi bức ảnh đã được thực hiện từ trước rồi, đó đã là một phần của feature engineering rồi). Một cách đơn giản thường được dùng là kéo dài ma trận 28x28 này để được 1 vector có số chiều 784. Trong cách này, các cột (hoặc hàng) của ma trận ảnh được đặt chồng lên (hoặc cạnh nhau) để được 1 vector dài. Vector dài này được trực tiếp sử dụng làm feature đưa vào các bộ classifier/clustering/regression/… Lúc này, giá trị của mỗi pixel ảnh được coi là một feature.\n",
    "\n",
    "Rõ ràng việc làm đơn giản này đã làm mất thông tin về không gian (spatial information) giữa các điểm ảnh, tuy nhiên, trong nhiều trường hợp, nó vẫn mang lại kết quả khả quan.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8ac2063",
   "metadata": {},
   "source": [
    "#### __Lựa chọn đặc trưng__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2717060d",
   "metadata": {},
   "source": [
    "<div align=\"justify\">\n",
    "\n",
    "Giả sử rằng các điểm dữ liệu có số features khác nhau (do kích thước dữ liệu khác nhau hay do một số feature mà điểm dữ liệu này có nhưng điểm dữ liệu kia lại không thu thập được), và số lượng features là cực lớn. Chúng ta cần chọn ra một số lượng nhỏ hơn các feature phù hợp với bài toán. Chọn thế nào và thế nào là phù hợp lại là một bài toán khác, tôi sẽ không bàn thêm ở đây.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c61ce2c",
   "metadata": {},
   "source": [
    "#### __Dimensionality reduction__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf7a03cf",
   "metadata": {},
   "source": [
    "<div align=\"justify\">\n",
    "\n",
    "Một phương pháp nữa tôi đã đề cập đó là làm giảm số chiều của dữ liệu để giảm bộ nhớ và khối lượng tính toán. Việc giảm số chiều này có thể được thực hiện bằng nhiều cách, trong đó random projection là cách đơn giản nhất. Tức chọn một ma trận chiếu (projection matrix) ngẫu nhiên (ma trận béo) rồi nhân nó với từng điểm dữ liệu (giả sử dữ liệu ở dạng vector cột) để được các vector có số chiều thấp hơn. Ví dụ, vector ban đầu có số chiều là 784, chọn ma trận chiếu có kích thước (100x784), khi đó nếu nhân ma trận chéo này với vector ban đầu, ta sẽ được một vector mới có số chiều là 100, nhỏ hơn số chiều ban đầu rất nhiều. Lúc này, có thể ta không có tên gọi cho mỗi feature nữa vì các feature ở vector ban đầu đã được trộn lẫn với nhau theo một tỉ lệ nào đó rồi lưu vào vector mới này. Mỗi thành phần của vector mới này được coi là một feature (không tên).\n",
    "\n",
    "Việc chọn một ma trận chiếu ngẫu nhiên đôi khi mang lại kết quả tệ không mong muốn vì thông tin bị mất đi quá nhiều. Một phương pháp được sử dụng nhiều để hạn chế lượng thông tin mất đi có tên là Principle Component Analysis sẽ được tôi trình bày sau đây khoảng 1-2 tháng.\n",
    "\n",
    "__Chú ý:__ Feature learning không nhất thiết phải làm giảm số chiều dữ liệu, đôi khi feature vector còn có số chiều lớn hơn raw data. Random projection cũng có thể làm được việc này nếu ma trận chiếu là một ma trận cao (số cột ít hơn số hàng).\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4c89277",
   "metadata": {},
   "source": [
    "#### __Bag-of-words__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2c28328",
   "metadata": {},
   "source": [
    "<div align=\"justify\">\n",
    "\n",
    "Hẳn rất nhiều bạn đã tự đặt câu hỏi: Với một văn bản thì feature vector sẽ có dạng như thế nào? Làm sao đưa các từ, các câu, đoạn văn ở dạng text trong các văn bản về một vector mà mỗi phần tử là một số?\n",
    "\n",
    "Có một phương pháp rất phổ biến giúp bạn trả lời những câu hỏi này. Phương pháp đó có tên là Bag of Words (BoW) (Túi đựng Từ).\n",
    "\n",
    "Vẫn theo thói quen, tôi bắt đầu bằng một ví dụ. Giả sử chúng ta có bài toán phân loại tin rác. Ta thấy rằng nếu một tin có chứa các từ khuyến mại, giảm giá, trúng thưởng, miễn phí, quà tặng, tri ân, … thì nhiều khả năng đó là một tin nhắn rác. Vậy phương pháp đơn giản nhất là đếm xem trong tin đó có bao nhiêu từ thuộc vào các từ trên, nếu nhiều hơn 1 ngưỡng nào đó thì ta quyết định đó là tin rác. (Tất nhiên bài toán thực tế phức tạp hơn nhiều khi các từ có thể được viết dưới dạng không dấu, hoặc bị cố tình viết sai chính tả, hoặc dùng ngôn ngữ teen). Với các loại văn bản khác nhau thì lượng từ liên quan tới từng chủ đề cũng khác nhau. Từ đó có thể dựa vào số lượng các từ trong từng loại để làm các vector đặc trưng cho từng văn bản.\n",
    "\n",
    "Tôi xin lấy ví dụ cụ thể hơn về cách tạo ra vector đặc trưng cho mỗi văn bản dựa trên BoW và xin được lấy tiếng Anh làm ví dụ (nguồn Bag of Words wiki. Tiếng Việt khó hơn vì một từ có thể có nhiều âm tiết, tiếng Anh thì thường cứ gặp dấu cách là kết thúc một từ).\n",
    "\n",
    "Giả sử chúng ta có hai văn bản đơn giản:\n",
    "\n",
    "```\n",
    "(1) John likes to watch movies. Mary likes movies too.\n",
    "```\n",
    "\n",
    "và\n",
    "\n",
    "```\n",
    "(2) John also likes to watch football games.\n",
    "```\n",
    "\n",
    "Dựa trên hai văn bản này, ta có danh sách các từ được sử dụng, được gọi là từ điển với 10 từ như sau:\n",
    "\n",
    "```\n",
    "[\"John\", \"likes\", \"to\", \"watch\", \"movies\", \"also\", \"football\", \"games\", \"Mary\", \"too\"]\n",
    "```\n",
    "\n",
    "Với mỗi văn bản, ta sẽ tạo ra một vector đặc trưng có số chiều bằng 10, mỗi phần tử đại diện cho số từ tương ứng xuất hiện trong văn bản đó. Với hai văn bản trên, ta sẽ có hai vector đặc trưng là:\n",
    "\n",
    "```\n",
    "(1) [1, 2, 1, 1, 2, 0, 0, 0, 1, 1]\n",
    "(2) [1, 1, 1, 1, 0, 1, 1, 1, 0, 0]\n",
    "```\n",
    "\n",
    "Văn bản (1) có 1 từ “John”, 2 từ “likes”, 0 từ “also”, 0 từ “football”, … nên ta thu được vector tương ứng như trên.\n",
    "\n",
    "Có một vài điều cần lưu ý trong BoW:\n",
    "\n",
    "- Với những ứng dụng thực tế, từ điền có nhiều hơn 10 từ rất nhiều, có thể đến một trăm nghìn hoặc cả triệu, như vậy vector đặc trưng thu được sẽ rất dài. Một văn bản chỉ có 1 câu, và 1 tiểu thuyết nghìn trang đều được biểu diễn bằng các vector có số chiều bằng 100 nghìn hoặc 1 triệu.\n",
    "- Có rất nhiều từ trong từ điển không xuất hiện trong một văn bản. Như vậy các vector đặc trưng thu được thường có rất nhiều phần tử bằng 0. Các vector có nhiều phần tử bằng 0 được gọi là sparse vector (sparse hiểu theo nghĩa là thưa thớt, rải rác, tôi xin phép chỉ sử dụng khái niệm này bằng tiếng Anh). Để việc lưu trữ được hiệu quả hơn, ta không lưu cả vector đó mà chỉ lưu vị trí của các phần tử khác 0 và giá trị tương ứng. Lưu ý: nếu có hơn 50% số phần tử khác 0, việc làm này lại phản tác dụng!\n",
    "- Thi thoảng có những từ hiếm gặp không nằm trong từ điển, vậy ta sẽ làm gì? Một cách thường được dùng là mở rộng vector đặc trưng thêm 1 phần tử, gọi là phẩn tử <Unknown>. Mọi từ không có trong từ điền đều được coi là <Unknown>.\n",
    "- Nghĩ kỹ một chút, những từ hiếm đôi khi lại mang những thông tin quan trọng nhất mà chỉ loại văn bản đó có. Đây là một nhược điểm của BoW. Có một phương pháp cải tiến khác giúp khắc phục nhược điểm này có tên là Term Frequency-Inverse Document Frequency (TF-IDF) dùng để xác định tầm quan trọng của một từ trong một văn bản dựa trên toàn bộ văn bản trong cơ sở dữ liệu (corpus). Bạn đọc muốn tìm hiểu thêm có thể xem 5 Algorithms Every Web Developer Can Use and Understand, section 5.\n",
    "- Nhược điểm lớn nhất của BoW là nó không mang thông tin về thứ tự của các từ. Cũng như sự liên kết giữa các câu, các đoạn văn trong văn bản. Ví dụ, ba câu sau đây: “Em yêu anh không?”, “Em không yêu anh”, và “Không, (nhưng) anh yêu em” khi được trích chọn đặc trưng bằng BoW sẽ cho ra ba vector giống hệt nhau, mặc dù ý nghĩa khác hẳn nhau.\n",
    "\n",
    "__Bonus:__ hình dưới đây là tần suất sử dụng các từ (coi mỗi âm tiết là một từ) trong Truyện Kiều (theo bản này) nếu ta chỉ sử dụng 30 từ có tần suất cao nhất.\n",
    "\n",
    "<div align=\"center\">\n",
    "\n",
    "![](../../images/bow-example.png)\n",
    "\n",
    "__Hình 2:__ Bag of Words cho Truyện Kiều với 30 từ có tần suất cao nhất.\n",
    "\n",
    "</div>\n",
    "\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
