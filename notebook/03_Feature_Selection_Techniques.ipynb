{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fde2ca05",
   "metadata": {},
   "source": [
    "<div align=\"justify\">\n",
    "\n",
    "In data science many times we encounter vast of features present in a dataset. But it is not necessary all features contribute equally in prediction that's where feature selection comes. It involves selecting a subset of relevant features from the original feature set to reduce the feature space while improving the model’s performance by reducing computational power. When a dataset has too many features, some may be irrelevant or add noise which can slow down training process and reduce accuracy but it helps in building simpler, faster and more accurate models and also helps in reducing overfitting.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdd4d61c",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "\n",
    "\n",
    "![](../images/image4.png)\n",
    "\n",
    "![](../images/image5.png)\n",
    "\n",
    "![](../images/image6.png)\n",
    "\n",
    "![](../images/image7.png)\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d55f9cc1",
   "metadata": {},
   "source": [
    "<div align=\"justify\">\n",
    "\n",
    "There are various algorithms used for feature selection and are grouped into three main categories and each one has its own strengths and trade-offs depending on the use case.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82e2cbbc",
   "metadata": {},
   "source": [
    "### __1. Filter Methods__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9d79f8e",
   "metadata": {},
   "source": [
    "<div align=\"justify\">\n",
    "\n",
    "Filter methods evaluate each feature independently with target variable. Feature with high correlation with target variable are selected as it means this feature has some relation and can help us in making predictions. These methods are used in the preprocessing phase to remove irrelevant or redundant features based on statistical tests (correlation) or other criteria.\n",
    "\n",
    "<div align=\"center\">\n",
    "\n",
    "![](../images/filter-methods-implementation.png)\n",
    "\n",
    "_Filter Methods Implementation_\n",
    "\n",
    "</div>\n",
    "\n",
    "<strong>Advantages</strong>\n",
    "\n",
    "- Quickly evaluate features without training the model.\n",
    "- Good for removing redundant or correlated features.\n",
    "\n",
    "<strong>Limitations:</strong> These methods don't consider feature interactions so they may miss feature combinations that improve model performance.\n",
    "\n",
    "Some techniques used are:  \n",
    "\n",
    "- __Information Gain:__ It is defined as the amount of information provided by the feature for identifying the target value and measures reduction in the entropy values. Information gain of each attribute is calculated considering the target values for feature selection.\n",
    "- __Chi-square test:__ It is generally used to test the relationship between categorical variables. It compares the observed values from different attributes of the dataset to its expected value.\n",
    "- __Fisher’s Score:__ It selects each feature independently according to their scores under Fisher criterion leading to a suboptimal set of features. The larger the Fisher’s score is, the better is the selected feature.\n",
    "- __Pearson’s Correlation Coefficient:__ It is a measure of quantifying the association between the two continuous variables and the direction of the relationship with its values ranging from -1 to 1.\n",
    "- __Variance Threshold:__ It is an approach where all features are removed whose variance doesn’t meet the specific threshold. By default this method removes features having zero variance. The assumption made using this method is higher variance features are likely to contain more information.\n",
    "- __Mean Absolute Difference:__ It is a method is similar to variance threshold method but the difference is there is no square in this method. This method calculates the mean absolute difference from the mean value.\n",
    "- __Dispersion ratio:__ It is defined as the ratio of the Arithmetic mean (AM) to that of Geometric mean (GM) for a given feature. Its value ranges from +1 to infinity as AM ≥ GM for a given feature. Higher dispersion ratio implies a more relevant feature.\n",
    "\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
